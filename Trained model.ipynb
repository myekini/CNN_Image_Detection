{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2c9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import utils \n",
    "from keras import backend as K\n",
    "import tensorflow.keras as K\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense\n",
    "from keras.layers import Dense, Activation, Dropout, Reshape, Permute, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e76fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "# Define data path\n",
    "data_path = 'Dataset'\n",
    "data_dir_list = os.listdir(data_path)  \n",
    "data_dir_list\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "num_channel = 3\n",
    "num_epoch = 90\n",
    "\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 5\n",
    "img_data_list = []\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    print(dataset)\n",
    "    #img_list = os.listdir(data_path + '/' + dataset)\n",
    "    excluded_files = [\".DS_Store\", \".Trashes\"]\n",
    "    \n",
    "    try:\n",
    "        img_list = [img for img in os.listdir(data_path + '/' + dataset) if img not in excluded_files]\n",
    "        print('Loaded the images of dataset-' + '{}\\n'.format(dataset))\n",
    "        for img in img_list:\n",
    "            # Your image processing code here\n",
    "            input_img = cv2.imread(data_path + '/' + dataset + '/' + img)\n",
    "            if input_img is not None:  # Check if the image is loaded successfully\n",
    "                input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "                input_img_resize = cv2.resize(input_img, (224, 224))\n",
    "                img_data_list.append(input_img_resize)\n",
    "    except NotADirectoryError:\n",
    "        print(f\"Ignoring hidden file: {dataset}/.DS_Store\")\n",
    "    \n",
    "\n",
    "\n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print(img_data.shape)\n",
    "\n",
    "\n",
    "# Set image data format to 'channels_last' (for TensorFlow backend)\n",
    "K.backend.set_image_data_format('channels_last')\n",
    "\n",
    "if num_channel == 3:\n",
    "    if K.backend.image_data_format() == 'channels_first':\n",
    "        img_data = np.expand_dims(img_data, axis=1)\n",
    "        print(img_data.shape)\n",
    "    else:\n",
    "        img_data = np.expand_dims(img_data, axis=3)\n",
    "        print(img_data.shape)\n",
    "else:\n",
    "    if K.backend.image_data_format() == 'channels_first':\n",
    "        img_data = np.rollaxis(img_data, 3, 1)\n",
    "        print(img_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "# Rest of your code goes here...\n",
    "\n",
    "# Assigning Labels & define the number of classes\n",
    "num_classes = 5\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,),dtype='int64')\n",
    "\n",
    "labels[0:1000]=0\n",
    "labels[1000:2000]=1\n",
    "labels[2000:3000]=2\n",
    "labels[3000:4000]=3\n",
    "labels[4000:5000]=4\n",
    "names = ['African_Almond', 'Avocado', 'Cashew', 'Guava','Mango']\n",
    "\n",
    "# Convert class labels to on-hot encoding\n",
    "Y = utils.to_categorical(labels, num_classes)\n",
    "\n",
    "# Shuffle and Split the dataset while maintaining the test size for each class\n",
    "x_by_class = defaultdict(list)\n",
    "y_by_class = defaultdict(list)\n",
    "\n",
    "for i, label in enumerate(Y):\n",
    "    x_by_class[np.argmax(label)].append(img_data[i])\n",
    "    y_by_class[np.argmax(label)].append(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    x_class = x_by_class[class_index]\n",
    "    y_class = y_by_class[class_index]\n",
    "    x_train_class, x_test_class, y_train_class, y_test_class = train_test_split(x_class, y_class, test_size=200/len(x_class), random_state=2)\n",
    "    X_train.extend(x_train_class)\n",
    "    X_test.extend(x_test_class)\n",
    "    y_train.extend(y_train_class)\n",
    "    y_test.extend(y_test_class)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Shuffle the training set\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2)\n",
    "\n",
    "print(\"X_train shape = {}\".format(X_train.shape))\n",
    "print(\"X_test shape = {}\".format(X_test.shape))\n",
    "\n",
    "image = X_train[36,:].reshape((224,224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76caf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Designing and training a CNN model in Keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16,(3,3),padding='same',kernel_initializer='he_normal',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer,metrics=[\"accuracy\"])\n",
    "\n",
    "import time\n",
    "\n",
    "# Start the timer for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train 0\n",
    "# Training\n",
    "hist = model.fit(X_train, y_train, batch_size=7, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# End the timer for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "print(\"Training Time: {:.2f} seconds\".format(training_time))\n",
    "\n",
    "# Start the timer for testing\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# End the timer for testing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate testing time\n",
    "testing_time = end_time - start_time\n",
    "print(\"Testing Time: {:.2f} seconds\".format(testing_time))\n",
    "model.save_weights('model_weights-leaf-2.weights.h5')  # Change to 'model_weights-leaf.weights.h5'\n",
    "\n",
    "model.save('model_keras_leaf_2.h5')\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test Accuracy:', score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71678f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizing losses and accuracy\n",
    "%matplotlib inline\n",
    "\n",
    "train_loss=hist.history['loss']\n",
    "val_loss=hist.history['val_loss']\n",
    "train_acc=hist.history['accuracy']\n",
    "val_acc=hist.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_acc))\n",
    "\n",
    "plt.plot(epochs,train_loss,'r', label='train_loss')\n",
    "plt.plot(epochs,val_loss,'b', label='val_loss')\n",
    "plt.title('train_loss vs val_loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs,train_acc,'r', label='train_acc')\n",
    "plt.plot(epochs,val_acc,'b', label='val_acc')\n",
    "plt.title('train_acc vs val_acc')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "test_image = X_test[0:1]\n",
    "print(test_image.shape)\n",
    "predictions = model.predict(test_image)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "print(predictions)\n",
    "print(predicted_class)\n",
    "print(y_test[0:1])\n",
    "\n",
    "\n",
    "image = test_image.reshape((224,224))\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d2214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featuremaps(model, layer_idx, X_batch):\n",
    "    get_activations = K.function([model.input, K.learning_phase()], [model.layers[layer_idx].output])\n",
    "    activations = get_activations([X_batch, 0])\n",
    "    return activations[0]\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)\n",
    "y_pred = np.argmax(Y_pred,axis=1)\n",
    "print(y_pred)\n",
    "\n",
    "target_names=['Class 0 (African_Almond)', 'Class 1 (Avocado)', 'Class 2 (Cashew)', 'Class 3 (Guava)','Class 4 (Mango)']\n",
    "print(classification_report(np.argmax(y_test,axis=1),y_pred,target_names=target_names))\n",
    "\n",
    "print('Confusion Matrix \\n')\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float32') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Confusion matrix with Normalization\")\n",
    "    else:\n",
    "        print('Confusion matrix without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion Matrix without Normalisation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf77865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'C:/Users/USER/Desktop/Data/Dataset\n",
    "test_img = cv2.imread('Dataset/Mango/105.jpg')\n",
    "if test_img is None:\n",
    "    print(\"Error: Failed to read the image file.\")\n",
    "else: \n",
    "    # Continue processing the image\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "    test_img = cv2.resize(test_img, (224, 224))\n",
    "    test_img = np.array(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfc7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape if necessary (check your model's input shape)\n",
    "new_image = np.expand_dims(test_img, axis=0)  # Add batch dimension\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(new_image)\n",
    "\n",
    "# Get the most likely class\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Print the predicted class name (optional)\n",
    "print(f\"Predicted Class: {target_names[predicted_class]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24dad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139804c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a preprocessed image named 'new_image'\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model architecture\n",
    "model_saved = load_model('model_keras_leaf.h5')\n",
    "\n",
    "# Load the checkpoint weights (replace with your checkpoint path)\n",
    "model_saved.load_weights('model_weights-leaf.weights.h5')\n",
    "\n",
    "# Make prediction on the new image\n",
    "predictions = model_saved.predict(new_image)\n",
    "\n",
    "# Optional: Extract the most likely class\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Print the predicted class (assuming you have class names)\n",
    "print(f\"Predicted Class: {target_names[predicted_class]}\")\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing data (assuming you have X_test and y_test)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
