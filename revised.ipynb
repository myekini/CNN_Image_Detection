{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import utils \n",
    "from keras import backend as K\n",
    "import tensorflow.keras as K\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense\n",
    "from keras.layers import Dense, Activation, Dropout, Reshape, Permute, BatchNormalization\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define constants\n",
    "PATH = os.getcwd()\n",
    "data_path = 'Dataset'\n",
    "img_rows = 224\n",
    "img_cols = 224\n",
    "num_channel = 3\n",
    "num_epoch = 90\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize lists to store image data\n",
    "img_data_list = []\n",
    "\n",
    "# Function to preprocess and load images\n",
    "def preprocess_images(data_path, img_rows, img_cols):\n",
    "    \"\"\"\n",
    "    Preprocess images by resizing and converting to grayscale.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the dataset directory.\n",
    "        img_rows (int): Height of the resized images.\n",
    "        img_cols (int): Width of the resized images.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Array containing preprocessed image data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_dir_list = os.listdir(data_path)\n",
    "        for dataset in data_dir_list:\n",
    "            logger.info(f\"Processing dataset: {dataset}\")\n",
    "            excluded_files = [\".DS_Store\", \".Trashes\"]\n",
    "            if dataset in excluded_files:\n",
    "                continue  # Skip hidden files\n",
    "\n",
    "            img_list = [img for img in os.listdir(os.path.join(data_path, dataset)) if img not in excluded_files]\n",
    "            logger.info(f\"Loaded {len(img_list)} images from dataset: {dataset}\")\n",
    "            \n",
    "            for img in img_list:\n",
    "                img_path = os.path.join(data_path, dataset, img)\n",
    "                input_img = cv2.imread(img_path)\n",
    "                if input_img is not None:\n",
    "                    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "                    input_img_resize = cv2.resize(input_img, (img_rows, img_cols))\n",
    "                    img_data_list.append(input_img_resize)\n",
    "                else:\n",
    "                    logger.warning(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred during image preprocessing: {str(e)}\")\n",
    "\n",
    "    return np.array(img_data_list)\n",
    "\n",
    "# Preprocess and load images\n",
    "img_data = preprocess_images(data_path, img_rows, img_cols)\n",
    "\n",
    "# Check if img_data is empty\n",
    "if len(img_data) == 0:\n",
    "    logger.error(\"No images loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Normalize image data\n",
    "img_data = img_data.astype('float32') / 255.0\n",
    "\n",
    "# Adjust image data format\n",
    "if num_channel == 3:\n",
    "    if img_data.shape[-1] != num_channel:\n",
    "        img_data = np.expand_dims(img_data, axis=-1)\n",
    "else:\n",
    "    if img_data.shape[-1] != num_channel:\n",
    "        logger.error(\"Invalid number of image channels.\")\n",
    "        exit()\n",
    "\n",
    "logger.info(f\"Preprocessed image data shape: {img_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 5\n",
    "names = ['African_Almond', 'Avocado', 'Cashew', 'Guava', 'Mango']\n",
    "\n",
    "# Assign labels to the images\n",
    "num_of_samples = img_data.shape[0]\n",
    "labels = np.ones((num_of_samples,), dtype='int64')\n",
    "labels[0:1000] = 0\n",
    "labels[1000:2000] = 1\n",
    "labels[2000:3000] = 2\n",
    "labels[3000:4000] = 3\n",
    "labels[4000:5000] = 4\n",
    "\n",
    "# Convert class labels to one-hot encoding\n",
    "Y = to_categorical(labels, num_classes)\n",
    "\n",
    "# Shuffle and split the dataset while maintaining the test size for each class\n",
    "x_by_class = defaultdict(list)\n",
    "y_by_class = defaultdict(list)\n",
    "\n",
    "for i, label in enumerate(Y):\n",
    "    x_by_class[np.argmax(label)].append(img_data[i])\n",
    "    y_by_class[np.argmax(label)].append(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    x_class = x_by_class[class_index]\n",
    "    y_class = y_by_class[class_index]\n",
    "    \n",
    "    try:\n",
    "        x_train_class, x_test_class, y_train_class, y_test_class = train_test_split(\n",
    "            x_class, y_class, test_size=200 / len(x_class), random_state=2\n",
    "        )\n",
    "        X_train.extend(x_train_class)\n",
    "        X_test.extend(x_test_class)\n",
    "        y_train.extend(y_train_class)\n",
    "        y_test.extend(y_test_class)\n",
    "    except ValueError:\n",
    "        print(f\"Skipping class {class_index}: Not enough samples for testing.\")\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Shuffle the training set\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2)\n",
    "\n",
    "# Log shapes of training and test sets\n",
    "print(\"X_train shape =\", X_train.shape)\n",
    "print(\"X_test shape =\", X_test.shape)\n",
    "\n",
    "# Visualize a sample image from the training set\n",
    "plt.imshow(X_train[36].reshape((224, 224)), cmap='gray')\n",
    "plt.title(names[np.argmax(y_train[36])])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define constants\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), padding='same', kernel_initializer='he_normal', input_shape=X_train.shape[1:]),\n",
    "    Activation('elu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
    "    Activation('elu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
    "    Activation('elu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal'),\n",
    "    Activation('elu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(256, kernel_initializer='he_normal'),\n",
    "    Activation('elu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, kernel_initializer='he_normal'),\n",
    "    Activation('elu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, kernel_initializer='he_normal'),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "training_time = end_time - start_time\n",
    "print(\"Training Time: {:.2f} seconds\".format(training_time))\n",
    "\n",
    "# Evaluate the model\n",
    "start_time = time.time()\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate testing time\n",
    "testing_time = end_time - start_time\n",
    "print(\"Testing Time: {:.2f} seconds\".format(testing_time))\n",
    "\n",
    "# Save the trained model\n",
    "model.save('model_keras_leaf_2.h5')\n",
    "\n",
    "# Print test loss and accuracy\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy * 100)\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize losses and accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, 'r', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on a sample test image\n",
    "test_image = X_test[0:1]\n",
    "predictions = model.predict(test_image)\n",
    "predicted_class_index = np.argmax(predictions, axis=1)\n",
    "predicted_class_probability = np.max(predictions)\n",
    "\n",
    "# Get the predicted and true class labels\n",
    "predicted_class_label = names[predicted_class_index[0]]\n",
    "true_class_index = np.argmax(y_test[0])\n",
    "true_class_label = names[true_class_index]\n",
    "\n",
    "# Display the test image\n",
    "plt.imshow(test_image.reshape((224, 224)), cmap='gray')\n",
    "plt.title(f\"Predicted Class: {predicted_class_label}, True Class: {true_class_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Log prediction results\n",
    "print(\"Test Image Shape:\", test_image.shape)\n",
    "print(\"Predicted Class Probabilities:\", predictions)\n",
    "print(\"Predicted Class Index:\", predicted_class_index)\n",
    "print(\"Predicted Class Label:\", predicted_class_label)\n",
    "print(\"True Class Label:\", true_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        # Read the image\n",
    "        test_img = cv2.imread(image_path)\n",
    "        \n",
    "        if test_img is None:\n",
    "            raise ValueError(\"Failed to read the image file.\")\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        test_img_gray = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Resize to 224x224\n",
    "        test_img_resized = cv2.resize(test_img_gray, (224, 224))\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        test_img_array = np.array(test_img_resized)\n",
    "        \n",
    "        return test_img_array\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "# Path to the test image file\n",
    "image_path = 'Dataset/Mango/105.jpg'\n",
    "\n",
    "# Load and preprocess the test image\n",
    "test_img = preprocess_image(image_path)\n",
    "\n",
    "if test_img is not None:\n",
    "    # Continue processing the image (e.g., make predictions)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the test image to match the model's input shape\n",
    "new_image = np.expand_dims(test_img, axis=0)\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(new_image)\n",
    "\n",
    "# Get the index of the predicted class\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# Get the predicted class label\n",
    "predicted_class_label = target_names[predicted_class_index]\n",
    "\n",
    "# Print the predicted class label\n",
    "print(\"Predicted Class:\", predicted_class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model_saved = load_model('model_keras_leaf.h5')\n",
    "\n",
    "# Load the checkpoint weights (replace with your checkpoint path)\n",
    "model_saved.load_weights('model_weights-leaf.weights.h5')\n",
    "\n",
    "# Make predictions on the new image\n",
    "predictions = model_saved.predict(new_image)\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# Print the predicted class label\n",
    "print(\"Predicted Class:\", target_names[predicted_class_index])\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "test_loss, test_acc = model_saved.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"\\nTest Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Get model predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "cnf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "print(cnf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names, title='Confusion Matrix without Normalization')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
